{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840628 sha256=c9050f5fe99109989742133f62a48e4e7eac86e69b5ad3e70fbb66b736781ed9\n",
      "  Stored in directory: /Users/vivek/Library/Caches/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructField, DateType\n",
    "from pyspark.sql.functions import col,month,year,quarter,count,countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product_id\",IntegerType(),True),\n",
    "    StructField(\"customer_id\",StringType(),True),\n",
    "    StructField(\"order_date\",DateType(),True),\n",
    "    StructField(\"location\",StringType(),True),\n",
    "    StructField(\"source_order\",StringType(),True), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.format(\"csv\").option(\"inferschema\",\"true\").schema(schema).load(\"/Users/vivek/Desktop/pyspark/pyspark project/sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+--------+------------+\n",
      "|product_id|customer_id|order_date|location|source_order|\n",
      "+----------+-----------+----------+--------+------------+\n",
      "|         1|          A|2023-01-01|   India|      Swiggy|\n",
      "|         2|          A|2022-01-01|   India|      Swiggy|\n",
      "|         2|          A|2023-01-07|   India|      Swiggy|\n",
      "|         3|          A|2023-01-10|   India|  Restaurant|\n",
      "|         3|          A|2022-01-11|   India|      Swiggy|\n",
      "|         3|          A|2023-01-11|   India|  Restaurant|\n",
      "|         2|          B|2022-02-01|   India|      Swiggy|\n",
      "|         2|          B|2023-01-02|   India|      Swiggy|\n",
      "|         1|          B|2023-01-04|   India|  Restaurant|\n",
      "|         1|          B|2023-02-11|   India|      Swiggy|\n",
      "|         3|          B|2023-01-16|   India|      zomato|\n",
      "|         3|          B|2022-02-01|   India|      zomato|\n",
      "|         3|          C|2023-01-01|   India|      zomato|\n",
      "|         1|          C|2023-01-01|      UK|      Swiggy|\n",
      "|         6|          C|2022-01-07|      UK|      zomato|\n",
      "|         3|          D|2023-02-16|      UK|  Restaurant|\n",
      "|         5|          D|2022-02-01|      UK|      zomato|\n",
      "|         3|          E|2023-02-01|      UK|  Restaurant|\n",
      "|         4|          E|2023-02-01|      UK|      Swiggy|\n",
      "|         4|          E|2023-02-07|      UK|  Restaurant|\n",
      "+----------+-----------+----------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.withColumn(\"order_year\",year(sales_df.order_date))\n",
    "sales_df = sales_df.withColumn(\"order_month\",month(sales_df.order_date))\n",
    "sales_df = sales_df.withColumn(\"order_quarter\",quarter(sales_df.order_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
      "|product_id|customer_id|order_date|location|source_order|order_year|order_month|order_quarter|\n",
      "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
      "|         1|          A|2023-01-01|   India|      Swiggy|      2023|          1|            1|\n",
      "|         2|          A|2022-01-01|   India|      Swiggy|      2022|          1|            1|\n",
      "|         2|          A|2023-01-07|   India|      Swiggy|      2023|          1|            1|\n",
      "|         3|          A|2023-01-10|   India|  Restaurant|      2023|          1|            1|\n",
      "|         3|          A|2022-01-11|   India|      Swiggy|      2022|          1|            1|\n",
      "|         3|          A|2023-01-11|   India|  Restaurant|      2023|          1|            1|\n",
      "|         2|          B|2022-02-01|   India|      Swiggy|      2022|          2|            1|\n",
      "|         2|          B|2023-01-02|   India|      Swiggy|      2023|          1|            1|\n",
      "|         1|          B|2023-01-04|   India|  Restaurant|      2023|          1|            1|\n",
      "|         1|          B|2023-02-11|   India|      Swiggy|      2023|          2|            1|\n",
      "|         3|          B|2023-01-16|   India|      zomato|      2023|          1|            1|\n",
      "|         3|          B|2022-02-01|   India|      zomato|      2022|          2|            1|\n",
      "|         3|          C|2023-01-01|   India|      zomato|      2023|          1|            1|\n",
      "|         1|          C|2023-01-01|      UK|      Swiggy|      2023|          1|            1|\n",
      "|         6|          C|2022-01-07|      UK|      zomato|      2022|          1|            1|\n",
      "|         3|          D|2023-02-16|      UK|  Restaurant|      2023|          2|            1|\n",
      "|         5|          D|2022-02-01|      UK|      zomato|      2022|          2|            1|\n",
      "|         3|          E|2023-02-01|      UK|  Restaurant|      2023|          2|            1|\n",
      "|         4|          E|2023-02-01|      UK|      Swiggy|      2023|          2|            1|\n",
      "|         4|          E|2023-02-07|      UK|  Restaurant|      2023|          2|            1|\n",
      "+----------+-----------+----------+--------+------------+----------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_schema = StructType([\n",
    "    StructField(\"product_id\",IntegerType(),True),\n",
    "    StructField(\"product_name\",StringType(),True),\n",
    "    StructField(\"price\",StringType(),True),\n",
    "\n",
    "])\n",
    "\n",
    "menu_df = spark.read.format(\"csv\").option(\"inferschema\",\"true\").schema(menu_schema).load(\"/Users/vivek/Desktop/pyspark/pyspark project/menu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         1|       PIZZA|  100|\n",
      "|         2|     Chowmin|  150|\n",
      "|         3|    sandwich|  120|\n",
      "|         4|        Dosa|  110|\n",
      "|         5|     Biryani|   80|\n",
      "|         6|       Pasta|  180|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "menu_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|sum(price)|\n",
      "+-----------+----------+\n",
      "|          A|    4260.0|\n",
      "|          B|    4440.0|\n",
      "|          C|    2400.0|\n",
      "|          D|    1200.0|\n",
      "|          E|    2040.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#total amount spent by each customer\n",
    "\n",
    "total_amount_spent = (sales_df.join(menu_df,\"product_id\").groupBy(\"customer_id\").agg({'price':'sum'}).orderBy(\"customer_id\"))\n",
    "total_amount_spent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|location|sum(price)|\n",
      "+--------+----------+\n",
      "|   India|    4860.0|\n",
      "|      UK|    7020.0|\n",
      "|     USA|    2460.0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#total spent by each country\n",
    "total_spent_by_each_food_country = (sales_df.join(menu_df,\"product_id\").groupBy(\"location\").agg({\"price\":\"sum\"}).orderBy(\"location\"))\n",
    "total_spent_by_each_food_country.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|order_month|sum(price)|\n",
      "+-----------+----------+\n",
      "|          1|    2960.0|\n",
      "|          2|    2730.0|\n",
      "|          3|     910.0|\n",
      "|          5|    2960.0|\n",
      "|          6|    2960.0|\n",
      "|          7|     910.0|\n",
      "|         11|     910.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#total amount of sales in each month\n",
    "\n",
    "sales = (sales_df.join(menu_df,\"product_id\").groupBy(\"order_month\").agg({'price':'sum'}).orderBy(\"order_month\"))\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|order_year|sum(price)|\n",
      "+----------+----------+\n",
      "|      2022|    4350.0|\n",
      "|      2023|    9990.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#total amount of sales in each month\n",
    "\n",
    "sales = (sales_df.join(menu_df,\"product_id\").groupBy(\"order_year\").agg({'price':'sum'}).orderBy(\"order_year\"))\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|product_id|product_name|product_count|\n",
      "+----------+------------+-------------+\n",
      "|         3|    sandwich|           48|\n",
      "|         2|     Chowmin|           24|\n",
      "|         1|       PIZZA|           21|\n",
      "|         4|        Dosa|           12|\n",
      "|         5|     Biryani|            6|\n",
      "|         6|       Pasta|            6|\n",
      "+----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how many times each product was purchased\n",
    "\n",
    "product_insight = (sales_df.join(menu_df,\"product_id\").groupBy(\"product_id\",\"product_name\").agg(count('product_id').alias('product_count')).orderBy(\"product_count\",ascending=0))\n",
    "product_insight.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------+\n",
      "|customer_id|count(DISTINCT order_date)|\n",
      "+-----------+--------------------------+\n",
      "|          E|                         5|\n",
      "|          B|                         6|\n",
      "|          D|                         1|\n",
      "|          C|                         3|\n",
      "|          A|                         6|\n",
      "+-----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#frequency of cust visited to restaurant\n",
    "freq_view = (sales_df.filter(sales_df.source_order=='Restaurant').groupBy(\"customer_id\").agg(countDistinct(\"order_date\")))\n",
    "freq_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|sum(price)|\n",
      "+-----------+----------+\n",
      "|          E|    2040.0|\n",
      "|          B|    4440.0|\n",
      "|          D|    1200.0|\n",
      "|          C|    2400.0|\n",
      "|          A|    4260.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spent by each customer\n",
    "\n",
    "\n",
    "spent_view = (sales_df.join(menu_df,\"product_id\").groupBy(\"customer_id\").agg({'price':'sum'}))\n",
    "spent_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|source_order|sum(price)|\n",
      "+------------+----------+\n",
      "|      zomato|    4920.0|\n",
      "|      Swiggy|    6330.0|\n",
      "|  Restaurant|    3090.0|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spent by order_source\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "order_view = (sales_df.join(menu_df,\"product_id\").groupBy(\"source_order\").agg({'price':'sum'}))\n",
    "order_view.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
